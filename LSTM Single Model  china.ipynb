{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# from eval import liHongEvaluator\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# from neural_model import TFNeuralNetworkRegressor\n",
    "# from get_onehotencoded_structure import get_structure1,dict_arm_to_structure1\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import log\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import json\n",
    "from keras.layers import LSTM\n",
    "# from conversions import get_structure1,dict_arm_to_structure1\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "# from getData import campaign_def\n",
    "import keras\n",
    "#from getData import campaign_def\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import RepeatVector\n",
    "from tflearn import DNN\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected \n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class liHongEvaluator():\n",
    "\n",
    "    def __init__(self, main_folder, output_column_name, print_params = {}, use_non_stationarity = False, sampling_fraction=0.1, stochastic_param = [1,0]):\n",
    "        self.main_folder = main_folder\n",
    "        self.df_original = pd.read_csv(main_folder,engine='python')\n",
    "#         self.df_original=self.df_original.drop(columns=['user_id','article_id'])\n",
    "        print(self.df_original.head())\n",
    "\n",
    "        all_columns = list(self.df_original.columns)\n",
    "        self.output_column_names = all_columns[62:]\n",
    "         \n",
    "\n",
    "        self.input_columns = list(set(all_columns) - set(self.output_column_names))\n",
    "        \n",
    "        print(self.input_columns)\n",
    "        self.arm_column_names = self.output_column_names\n",
    "        self.logging_frequency = 1000\n",
    "        self.number_features_in_context = len(self.input_columns)\n",
    "        self.no_oflist_arms = len(self.output_column_names)\n",
    "        total_arms =40\n",
    "\n",
    "        #For testing, Comment out\n",
    "        self.count_not_found = 0\n",
    "        self.found_in_exclusion = 0\n",
    "        self.found_in_set = 0\n",
    "        self.flag = True\n",
    "        x = self.df_original[self.input_columns].values\n",
    "        x_scaled=pd.DataFrame(x,columns = self.input_columns)\n",
    "        self.unique = self.df_original[self.output_column_names].drop_duplicates()\n",
    "        self.unique = self.unique.reset_index(drop=True)\n",
    "\n",
    "        self.y=self.df_original[self.output_column_names].values\n",
    "        #To simulate stochasticity set this paramater\n",
    "        self.stochastic_param = stochastic_param\n",
    "        x_tr, x_test , y_tr, y_test = train_test_split(x_scaled,self.y,test_size=0.3,shuffle=False)\n",
    "        x_train ,x_valid,y_train, y_valid = train_test_split(x_tr,y_tr,test_size=0.3,shuffle=False)\n",
    " \n",
    "        self.x_training = x_train.values.tolist() #list(map(float,x_train.values))#pd.DataFrame(x_tr, columns = self.input_columns)\n",
    "        self.y_training = y_train #pd.DataFrame(y_tr, columns = [self.output_column_names])\n",
    "        self.x_valid=x_valid.values.tolist()\n",
    "        self.y_valid=y_valid\n",
    "        self.x_testing =x_test.values.tolist()#pd.DataFrame(x_test, columns = self.input_columns)\n",
    "        self.y_testing = y_test# pd.DataFrame(y_test, columns = [self.output_column_names])\n",
    "        x_train_df = pd.DataFrame(x_tr, columns = self.input_columns)\n",
    "        x_test_df = pd.DataFrame(x_test, columns = self.input_columns)\n",
    "        \n",
    "        self.e=0.2\n",
    "        self.decay=0.998\n",
    "        \n",
    "        print ('x_traing',self.x_training[0])\n",
    "        print ('x_valid',self.x_valid[0])\n",
    "        print ('x_test' ,self.x_testing[0])\n",
    "\n",
    "\n",
    "        random.seed(9001)\n",
    "\n",
    "        self.non_stationarity_steps = 25000\n",
    "        \n",
    "    def get_size_input(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def get_no_oflist_arms(self):\n",
    "        return self.no_oflist_arms\n",
    "\n",
    "    def get_dataset_shape(self):\n",
    "        return self.df.shape\n",
    "\n",
    "    def get_number_features_in_context(self):\n",
    "        return self.number_features_in_context\n",
    "\n",
    "    #rename to get_one_hot_structure_array\n",
    "    def get_structure(self):\n",
    "#         print(np.unique(self.y))\n",
    "        structure=self.unique\n",
    "        #print 'structure', structure\n",
    "        return structure\n",
    "    #rename to get_one_hot_structure\n",
    "    def dict_arm_to_structure(self):\n",
    "        dict_str ={}\n",
    "        y_unique = self.get_structure()\n",
    "        print (y_unique.head())\n",
    "        for i ,r in y_unique.iterrows():\n",
    "            dict_str[i] = r.values.tolist()\n",
    "        return dict_str\n",
    "\n",
    "    def get_exclusions(self):\n",
    "        arms_set_dict ,self.exclusion_dict = campaign_def(self.main_folder)\n",
    "        return self.exclusion_dict\n",
    "\n",
    "\n",
    "    def evaluate(self,policy):\n",
    "        \n",
    "        dict_arm_structure = self.dict_arm_to_structure()\n",
    "        score=0\n",
    "        total_arms=40\n",
    "        start_time = datetime.now()\n",
    "        q=0\n",
    "        res_rew=[]\n",
    "        rew_est=0\n",
    "        count = 0\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\"now training model\")\n",
    "        for state, right_arm in zip(self.x_training,self.y_training):\n",
    "            \n",
    "            q+=1\n",
    "            if(q%100 == 0):\n",
    "                print('Hey there ', q)\n",
    "                res_rew.append(rew_est)\n",
    "                rew_est=0\n",
    "            \n",
    "            \n",
    "            import random\n",
    "            rand = random.uniform(0, 1)\n",
    "            \n",
    "            action_indices = []\n",
    "            if(rand < self.e):\n",
    "                self.e=self.e*self.decay\n",
    "                selected_action =[]\n",
    "                \n",
    "                while len(selected_action) <1:\n",
    "                    random_index = np.random.randint(0,total_arms)\n",
    "                    temp = self.unique.iloc[random_index].values.tolist()\n",
    "                    if(temp not in selected_action):\n",
    "                        selected_action.append(temp)\n",
    "                        action_indices.append(random_index)\n",
    "            else:        \n",
    "            \n",
    "                action_indices, selected_action = policy.get_action(state)\n",
    "            count=count+1\n",
    "            \n",
    "            if count % self.logging_frequency == 0 :\n",
    "                print('training set evaluated %d' % count)\n",
    "            reward=0\n",
    "\n",
    "            for action_index, arm in zip(action_indices, selected_action) :\n",
    "                reward=0\n",
    "                if arm == right_arm.tolist():\n",
    "\n",
    "                    reward = 1\n",
    "                    rew_est+=1\n",
    "\n",
    "                state2 =list(state) + (arm)\n",
    "                policy.update_model(reward,state2,arm,True, action_index)\n",
    "\n",
    "            \n",
    "            #print(\"!!!\", right_arm)\n",
    "#             state1 = list(state) + list(right_arm)\n",
    "#             policy.update_model(1,state1,right_arm,True)\n",
    "            \n",
    "#             import random\n",
    "#             random_sample = list(self.unique.iloc[random.randint(0,39)])\n",
    "#             if random_sample!=list(right_arm):\n",
    "#                 policy.update_model(0,state + random_sample,random_sample, True)\n",
    "\n",
    "#             pairs = list(zip(self.x_training,self.y_training))  # make pairs out of the two lists\n",
    "#             pairs = random.sample(pairs, 1)  # pick 3 random pairs\n",
    "#             A1, B1 = zip(*pairs)  # separate the pairs\n",
    "#             for k in range(1):\n",
    "#                 if(list(B1[k])!=list(right_arm)):\n",
    "#                     state1 = list(A1[k]) + list(B1[k])\n",
    "#                     policy.update_model(0,state1,B1[k],True)\n",
    "                    \n",
    "            \n",
    "#             for index, row in self.unique.iterrows():\n",
    "#                 row = row.to_numpy()\n",
    "#                 state2 =list( state) + list(row)\n",
    "#                 #print(state)\n",
    "#                 if(list(row)==list(right_arm)):\n",
    "#                     policy.update_model(1,state2,right_arm,True)\n",
    "#                 else:\n",
    "#                     policy.update_model(0,state2,row,True)\n",
    "            \n",
    "# # # #             print('state',state)\n",
    "            \n",
    "# # #         policy.train_model(self.x_training, self.y_training)\n",
    "        print(\"model trained\")\n",
    "        \n",
    "        count = 0\n",
    "        for state, right_arm in zip(self.x_valid, self.y_valid):\n",
    "            \n",
    "            q+=1\n",
    "            if(q%100 == 0):\n",
    "                res_rew.append(rew_est)\n",
    "                rew_est=0\n",
    "\n",
    "            import random\n",
    "            rand = random.uniform(0, 1)\n",
    "\n",
    "            action_indices = []\n",
    "            if(rand < self.e):\n",
    "                self.e=self.e*self.decay\n",
    "                selected_action =[]\n",
    "                \n",
    "                while len(selected_action) <1:\n",
    "                    random_index = np.random.randint(0,total_arms)\n",
    "                    temp = self.unique.iloc[random_index].values.tolist()\n",
    "                    if(temp not in selected_action):\n",
    "                        selected_action.append(temp)\n",
    "                        action_indices.append(random_index)\n",
    "            else:        \n",
    "            \n",
    "                action_indices, selected_action = policy.get_action(state)\n",
    "            \n",
    "            count=count+1\n",
    "            if count % self.logging_frequency == 0 :\n",
    "                print('validation set evaluated %d' % count)\n",
    "            reward=0\n",
    "\n",
    "\n",
    "            for action_index, arm in zip(action_indices, selected_action) :\n",
    "                reward=0\n",
    "                if arm == right_arm.tolist():\n",
    "\n",
    "                    reward = 1\n",
    "                    rew_est+=1\n",
    "\n",
    "                state2 =list(state) + (arm)\n",
    "                policy.update_model(reward,state2,arm,True, action_index)\n",
    "\n",
    "        \n",
    "        \n",
    "##############################################TESTING##############################################\n",
    "        \n",
    "        \n",
    "        count =0\n",
    "        \n",
    "        precision=0\n",
    "        dict_recall= {}\n",
    "        dict_impressions = {}\n",
    "        dict_clicks = {}\n",
    "        \n",
    "        for k in self.unique.values:\n",
    "            dict_impressions[tuple(k)] =0\n",
    "            dict_clicks[tuple(k)]=0\n",
    "        \n",
    "        ndgc=0\n",
    "        \n",
    "        for state, right_arm in zip(self.x_testing, self.y_testing):\n",
    "                                    \n",
    "            import random\n",
    "            rand = random.uniform(0, 1)\n",
    "\n",
    "            action_indices = []\n",
    "            if(rand < self.e):\n",
    "                self.e=self.e*self.decay\n",
    "                selected_action =[]\n",
    "                \n",
    "                while len(selected_action) <1:\n",
    "                    random_index = np.random.randint(0,total_arms)\n",
    "                    temp = self.unique.iloc[random_index].values.tolist()\n",
    "                    if(temp not in selected_action):\n",
    "                        selected_action.append(temp)\n",
    "                        action_indices.append(random_index)\n",
    "            else:        \n",
    "            \n",
    "                action_indices, selected_action = policy.get_action(state)\n",
    "                \n",
    "            q+=1\n",
    "            if(q%100 == 0):\n",
    "                res_rew.append(rew_est)\n",
    "                rew_est=0\n",
    "            \n",
    "\n",
    "\n",
    "            y_value = right_arm\n",
    "    \n",
    "            \n",
    "\n",
    "            count=count+1\n",
    "            self.stime = datetime.now()\n",
    "            if count % self.logging_frequency == 0 :\n",
    "                print('testing set evaluated %d' % count)\n",
    "# #             print('initial', len(state))\n",
    "            \n",
    "            match_found = False\n",
    "            ps =0\n",
    "            ps_sum=0\n",
    "            i=0\n",
    "            num_ndgc=0\n",
    "            d_ndgc=0\n",
    "            k_ndgc=0\n",
    "            for action_index, arm in zip(action_indices, selected_action) :\n",
    "                    reward=0\n",
    "                    dict_impressions[tuple(arm)]+=1\n",
    "\n",
    "                    if arm == right_arm.tolist():\n",
    "                        rew_est +=1\n",
    "\n",
    "                        match_found = True\n",
    "                        reward = 1\n",
    "                        score = score+1\n",
    "\n",
    "                        ps+=1\n",
    "                        ps_sum += ps/(i+1)\n",
    "                        i=i+1\n",
    "                        \n",
    "                        if(i+1==1):\n",
    "                            d_ndgc+=1\n",
    "                        else:\n",
    "                            d_ndgc+=1/(log(i+1))\n",
    "                        num_ndgc = num_ndgc+1\n",
    "                        \n",
    "                        dict_clicks[tuple(y_value)] +=1\n",
    "\n",
    "\n",
    "                    state2 =list(state) + (arm)\n",
    "                    policy.update_model(reward,state2,arm,True,action_index)\n",
    "                    \n",
    "            if(match_found):\n",
    "                precision =precision + ps_sum\n",
    "                \n",
    "            for i in range(num_ndgc):\n",
    "                if(i+1 == 1):\n",
    "                    k_ndgc+=1\n",
    "                else:\n",
    "                    k_ndgc+= 1/(log(i+1))\n",
    "            \n",
    "            if(k_ndgc!=0):\n",
    "                ndgc = ndgc + (d_ndgc/k_ndgc)\n",
    "            \n",
    "\n",
    "\n",
    "        print('accuracy %f' % ( float(score) / count ))\n",
    "        \n",
    "        ctr = 0 \n",
    "        for key, value in dict_impressions.items():\n",
    "            if(value!=0):\n",
    "                ctr += dict_clicks[key]/value\n",
    "        print('ctr',ctr/len(dict_impressions.keys()))\n",
    "        \n",
    "        with open('result_reward_single_news_ts1.pickle', 'wb') as handle:\n",
    "            pickle.dump(res_rew, handle)\n",
    "        \n",
    "        print (\"precision\")\n",
    "        print (precision/len(self.x_testing))\n",
    "        \n",
    "                \n",
    "        print (\"NDGC\")\n",
    "        print (ndgc/len(self.x_testing))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/anubha/Seagate Expansion Drive/china'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "data_path = '/media/anubha/Seagate Expansion Drive/china'\n",
    "class TFNeuralNetworkRegressor():\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, number_features_in_context, number_features_in_structure):\n",
    "        \n",
    "        \n",
    "        self.pre_model = Sequential()\n",
    "        self.pre_model.add(LSTM(32, batch_input_shape=(4,1,number_features_in_context+number_features_in_structure),stateful=True,name='layer1', return_sequences=True))\n",
    "        #self.pre_model.add(RepeatVector(1))\n",
    "        self.pre_model.add(LSTM((number_features_in_context+number_features_in_structure), return_sequences=True, stateful = True)) \n",
    "        self.pre_model.add(Dense((number_features_in_context+number_features_in_structure),activation ='sigmoid'))\n",
    "        self.pre_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#         checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=0)\n",
    "        print(self.pre_model.summary())\n",
    "    \n",
    "        self.model = Sequential()\n",
    "        #First Hidden Layer\n",
    "#         self.model.add(LSTM(120, batch_input_shape=(100,number_features_in_context+number_features_in_structure,1), stateful=True))\n",
    "\n",
    "        self.model.add(Dense(60, activation='tanh', kernel_initializer='random_normal', input_shape=(32,)))\n",
    "        \n",
    "        \n",
    "#         self.model.add(Dense(80, activation='tanh', kernel_initializer='random_normal'))\n",
    "        \n",
    "        self.model.add(Dense(30, activation='tanh', kernel_initializer='random_normal'))\n",
    "        \n",
    "\n",
    "        self.model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "        # For a binary classification problem\n",
    "        \n",
    "        self.model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    \n",
    "        layer_name = 'layer1'\n",
    "        self.intermediate_layer_model = Model(inputs=self.pre_model.input,outputs=self.pre_model.get_layer(layer_name).output)\n",
    "        \n",
    "        self.intermediate_batch_output_history = None\n",
    "       \n",
    "    def fit(self, X, Y):\n",
    "        X = X.reshape(X.shape[0],1,X.shape[1])\n",
    "#         intermediate_output = self.intermediate_layer_model.predict(data)\n",
    "        self.pre_model.fit(X,X,batch_size=4, verbose=0)\n",
    "        intermediate_output = self.intermediate_layer_model.predict(X,batch_size=4)\n",
    "        intermediate_output = intermediate_output.reshape((intermediate_output.shape[0], np.prod(intermediate_output.shape[1:])))\n",
    "        \n",
    "        self.model.fit(intermediate_output, Y, epochs=1, verbose=0)\n",
    "        \n",
    "        \n",
    "   \n",
    "    def partial_fit(self, X,Y, action_index):\n",
    "        \n",
    "        if self.intermediate_batch_output_history is None:\n",
    "            \n",
    "            aux = np.concatenate([X]*40,axis = 0)\n",
    "            aux = aux.reshape(aux.shape[0], 1,aux.shape[1])\n",
    "            intermediate_output = self.intermediate_layer_model.predict(aux,batch_size=4)\n",
    "            intermediate_output = intermediate_output.reshape((intermediate_output.shape[0], np.prod(intermediate_output.shape[1:])))\n",
    "            self.intermediate_batch_output_history = intermediate_output\n",
    "            print('intermediate output shape',intermediate_output.shape)\n",
    "        self.model.fit(np.array([self.intermediate_batch_output_history[action_index]]), Y, epochs=10, verbose=0)\n",
    "   \n",
    "    def predict(self, X):\n",
    "        #print('input shape', X.shape)\n",
    "        X = X.reshape(X.shape[0], 1,X.shape[1])\n",
    "        intermediate_output = self.intermediate_layer_model.predict(X,batch_size=4)\n",
    "        \n",
    "#       print('output shape', intermediate_output.shape)\n",
    "#         intermediate_from_pre_model = self.pre_model.get_layer('lstm_22').output\n",
    "        intermediate_output = intermediate_output.reshape((intermediate_output.shape[0], np.prod(intermediate_output.shape[1:])))\n",
    "        self.intermediate_batch_output_history = intermediate_output\n",
    "        return self.model.predict(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_arm_to_structure1():\n",
    "    with open('total_comb_structured_dict') as f:\n",
    "        data = json.load(f)\n",
    "        data1={}\n",
    "        for i in data.keys():\n",
    "            j=str(i)\n",
    "            data1[j]=data[i]\n",
    "        return data1\n",
    "\n",
    "def get_structure1():\n",
    "\ttotal_comb_structured_dict=dict_arm_to_structure1()\n",
    "\ttotal_structure_array= np.asarray(list(total_comb_structured_dict.values()))\n",
    "\treturn total_structure_array\n",
    "\n",
    "def num_to_key():\n",
    "    with open('dict_structure_to_arm') as f:\n",
    "        data = json.load(f)\n",
    "        data1={}\n",
    "        for i in data.keys():\n",
    "\n",
    "            j=str(i)\n",
    "            data1[j]=data[i]\n",
    "        return data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#150 -50-0.7 [150 - 100-50]\n",
    "class single_model_policy():\n",
    "    #Taking batch sizes of 1000\n",
    "    def __init__(self, number_features_in_context, structure, epsilon, training_dropout, prediction_dropout,batch_size = 100):\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "        self.last_state = None\n",
    "        self.num_arms = structure.shape[0]\n",
    "        self.structure_features = structure.shape[1]\n",
    "        self.epsilon = epsilon\n",
    "        self.experience = {}\n",
    "        self.i=0\n",
    "        self.total_features = number_features_in_context + self.structure_features\n",
    "        self.train_batch_size=120\n",
    "        self.mini_batch=40\n",
    "        dict_str ={}\n",
    "        self.cnt=0\n",
    "      \n",
    "        for i ,r in structure.iterrows():\n",
    "            dict_str[i] = r.values.tolist()\n",
    "            \n",
    "        \n",
    "     \n",
    "    \n",
    "        self.structure = dict_str\n",
    "#         print (self.structure)\n",
    "#         self.batch_size = batch_size\n",
    "        self.last_s=[]\n",
    "        self.last_r=[]\n",
    "        #if self.batch_size:\n",
    "        self.iter = 0\n",
    "        self.dict_update_hist = {}\n",
    "#         print(self.structure)\n",
    "        # self.model=TFNeuralNetworkRegressor(self.total_features, training_dropout, prediction_dropout)\n",
    "        self.model=TFNeuralNetworkRegressor(number_features_in_context, self.structure_features)\n",
    "        \n",
    "\n",
    "\n",
    "    #Choose best action for given context\n",
    "    def get_action(self, context):\n",
    "        #action should be category ID\n",
    "        best_action = self.get_best_arm(context)\n",
    "        arms=[]\n",
    "        for i in best_action:\n",
    "            arms.append(self.structure[i])\n",
    "            \n",
    "        return best_action, arms\n",
    "\n",
    "    #Loop over the structures of every arm for a given context and return the best prediction\n",
    "    def get_best_arm(self,context):\n",
    "\n",
    "        best_score = 0\n",
    "        best_arm = None\n",
    "\n",
    "        inputs = []\n",
    "        arms = []\n",
    "        for arm,structure in self.structure.items():\n",
    "#             print((structure))\n",
    "#             print (list())\n",
    "            state = list(context) + list(structure)\n",
    "#             print(len(state))\n",
    "            inputs.append(state)\n",
    "            arms.append(arm)\n",
    "\n",
    "        scores = [ elt[0] for elt in list(self.model.predict(np.array(inputs))) ]\n",
    "        #print(scores)\n",
    "        n=0\n",
    "        select_arm=[]\n",
    "        s = sorted(range(len(scores)),reverse = True,  key=lambda k: scores[k])\n",
    "\n",
    "        return s[:1]\n",
    "\n",
    "    \n",
    "    def train_model(self, x_train, y_train):\n",
    "        self.model.fit(x_train,y_train)\n",
    "    #Update model\n",
    "\n",
    "    def update_model(self, reward,last_state=[] ,last_action=[],update_in_train=False, action_index = 0):\n",
    "        #Immediate Updates\n",
    "        if(len(last_action) != 0):\n",
    "            self.last_action = last_action\n",
    "        if(len(last_state) !=0):\n",
    "            self.last_state = last_state\n",
    "        self.last_reward = reward\n",
    "        self.model.partial_fit(np.array([self.last_state]), np.array([[self.last_reward]]), action_index)\n",
    "#         print (batch_size)\n",
    "        if update_in_train:\n",
    "            if(len(self.last_s)!=120):\n",
    "#             if(self.iter == 0 or self.iter%self.train_batch_size != 0):\n",
    "#                 if(reward==0):\n",
    "#                     self.cnt+=1\n",
    "                    \n",
    "#                 if(self.cnt<int(0.5*self.train_batch_size) or reward==1):\n",
    "\n",
    "#                     if(self.last_state not in self.last_s):\n",
    "                self.last_s.append(self.last_state)\n",
    "                self.last_r.append(self.last_reward)\n",
    "\n",
    "            else:\n",
    "\n",
    "                for iteration in range(1,10):\n",
    "                    if(iteration%500==0):\n",
    "                        print(\"Iteration : %d\" % iteration)\n",
    "                    num_to_sample = min(len(self.last_s), self.mini_batch)\n",
    "                    random_indices=random.sample(range(0,len(self.last_s)),num_to_sample)\n",
    "                    x_train = []\n",
    "                    y_train = []\n",
    "                    for index in random_indices:\n",
    "\n",
    "                        x_train.append(self.last_s[index])\n",
    "                        y_train.append([self.last_r[index]])\n",
    "\n",
    "                    self.model.fit(np.array(x_train),np.array(y_train))\n",
    "                self.last_s=[]\n",
    "                self.last_r=[]\n",
    "                self.cnt=0\n",
    "            self.iter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frequency_rating    counts  activeness    rating  timestamp  longitude  \\\n",
      "0          0.174419  0.016938     1.00000  0.872302   0.000000   0.904337   \n",
      "1          0.174419  0.163279     1.00000  0.872302   0.000099   0.927004   \n",
      "2          0.906977  0.163279     0.00000  0.140288   0.000099   0.927004   \n",
      "3          1.000000  0.163279     0.96281  0.968562   0.000099   0.927004   \n",
      "4          0.992248  0.163279     1.00000  0.998801   0.000099   0.927004   \n",
      "\n",
      "   latitude  is_installed       age  phone_brand_0  ...  6.66657E+018  \\\n",
      "0  0.573701             1  0.177419              0  ...             0   \n",
      "1  0.446858             1  0.161290              0  ...             0   \n",
      "2  0.446858             1  0.161290              0  ...             0   \n",
      "3  0.446858             1  0.161290              0  ...             0   \n",
      "4  0.446858             1  0.161290              0  ...             0   \n",
      "\n",
      "   7.16711E+018  7.31625E+018  7.34834E+018  7.35097E+018  7.46008E+018  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "2             0             0             0             0             0   \n",
      "3             0             0             0             0             0   \n",
      "4             0             0             0             0             0   \n",
      "\n",
      "   8.5572E+018  8.69396E+018  8.94867E+018  9.11246E+018  \n",
      "0            0             0             0             0  \n",
      "1            0             0             0             0  \n",
      "2            0             0             0             0  \n",
      "3            0             0             0             0  \n",
      "4            0             1             0             0  \n",
      "\n",
      "[5 rows x 169 columns]\n",
      "['frequency_rating', 'phone_brand_1', 'is_installed', 'group_6', 'rating', 'phone_brand_17', 'phone_brand_2', 'phone_brand_32', 'timestamp', 'phone_brand_31', 'phone_brand_13', 'phone_brand_33', 'phone_brand_27', 'phone_brand_37', 'group_5', 'phone_brand_14', 'group_2', 'gender_1', 'group_0', 'group_9', 'age', 'phone_brand_4', 'phone_brand_0', 'phone_brand_36', 'phone_brand_35', 'phone_brand_9', 'phone_brand_5', 'phone_brand_29', 'is_active_1', 'phone_brand_12', 'phone_brand_6', 'phone_brand_24', 'phone_brand_8', 'phone_brand_7', 'phone_brand_25', 'group_8', 'longitude', 'phone_brand_30', 'phone_brand_26', 'phone_brand_11', 'phone_brand_15', 'group_3', 'gender_0', 'phone_brand_3', 'group_10', 'phone_brand_23', 'phone_brand_16', 'group_1', 'counts', 'phone_brand_18', 'activeness', 'group_11', 'phone_brand_22', 'phone_brand_34', 'phone_brand_28', 'latitude', 'group_4', 'phone_brand_10', 'group_7', 'phone_brand_19', 'phone_brand_21', 'phone_brand_20']\n",
      "x_traing [0.174418604651163, 0.0, 1.0, 0.0, 0.872302158273381, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.17741935483871, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.904337042268077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.016937669376693998, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.573700543056633, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "x_valid [0.156330749354005, 0.0, 1.0, 0.0, 0.02418065547562, 0.0, 0.0, 0.0, 0.4545544948289579, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.30645161290322603, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.95635399083675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7546547711404191, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "x_test [0.098837209302326, 0.0, 1.0, 0.0, 0.860611510791367, 0.0, 0.0, 0.0, 0.6689538583929993, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.193548387096774, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.9563195425264392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002710027100271, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.579712955779674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Created Evaluator\n"
     ]
    }
   ],
   "source": [
    "training_dropout, prediction_dropout =  0.5, 0.5\n",
    "epsilon = 0.00001\n",
    "\n",
    "main_folder = 'china_structure27.1.20.csv'\n",
    "\n",
    "target_evaluator = liHongEvaluator(main_folder, ['article_id'] , {}, False, 0.1, [1,0])\n",
    "\n",
    "print(\"Created Evaluator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(main_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure= target_evaluator.get_structure()\n",
    "number_features_in_context = target_evaluator.get_number_features_in_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (LSTM)                (4, 1, 32)                25856     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (4, 1, 169)               136552    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (4, 1, 169)               28730     \n",
      "=================================================================\n",
      "Total params: 191,138\n",
      "Trainable params: 191,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 60)                1980      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 3,841\n",
      "Trainable params: 3,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "smp = single_model_policy(number_features_in_context, structure, epsilon, training_dropout, prediction_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    like groupon  1 free  1 reputation  And the Church  Bank financing  \\\n",
      "0              0       0             0               0               0   \n",
      "1              0       0             0               1               0   \n",
      "2              0       0             0               0               0   \n",
      "3              0       0             0               0               0   \n",
      "4              0       0             0               0               0   \n",
      "\n",
      "   Buy class  Consumer Finance  Cozy 1  Custom label  Customized 1  ...  \\\n",
      "0          0                 0       0             0             0  ...   \n",
      "1          0                 0       0             1             0  ...   \n",
      "2          0                 0       0             0             0  ...   \n",
      "3          0                 0       0             0             0  ...   \n",
      "4          0                 0       0             0             0  ...   \n",
      "\n",
      "   6.66657E+018  7.16711E+018  7.31625E+018  7.34834E+018  7.35097E+018  \\\n",
      "0             0             0             0             0             0   \n",
      "1             0             0             0             0             0   \n",
      "2             0             0             0             0             0   \n",
      "3             0             0             0             0             0   \n",
      "4             0             0             0             0             0   \n",
      "\n",
      "   7.46008E+018  8.5572E+018  8.69396E+018  8.94867E+018  9.11246E+018  \n",
      "0             0            0             0             0             0  \n",
      "1             0            0             0             0             0  \n",
      "2             0            0             0             0             0  \n",
      "3             0            0             1             0             0  \n",
      "4             0            0             0             0             0  \n",
      "\n",
      "[5 rows x 107 columns]\n",
      "now training model\n",
      "intermediate output shape (40, 32)\n",
      "WARNING:tensorflow:From /home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Hey there  100\n",
      "Hey there  200\n",
      "Hey there  300\n",
      "Hey there  400\n",
      "Hey there  500\n",
      "Hey there  600\n",
      "Hey there  700\n",
      "Hey there  800\n",
      "Hey there  900\n",
      "Hey there  1000\n",
      "training set evaluated 1000\n",
      "Hey there  1100\n",
      "Hey there  1200\n",
      "Hey there  1300\n",
      "Hey there  1400\n",
      "Hey there  1500\n",
      "Hey there  1600\n",
      "Hey there  1700\n",
      "Hey there  1800\n",
      "Hey there  1900\n",
      "Hey there  2000\n",
      "training set evaluated 2000\n",
      "Hey there  2100\n",
      "Hey there  2200\n",
      "Hey there  2300\n",
      "Hey there  2400\n",
      "Hey there  2500\n",
      "Hey there  2600\n",
      "Hey there  2700\n",
      "Hey there  2800\n",
      "Hey there  2900\n",
      "Hey there  3000\n",
      "training set evaluated 3000\n",
      "Hey there  3100\n",
      "Hey there  3200\n",
      "Hey there  3300\n",
      "Hey there  3400\n",
      "Hey there  3500\n",
      "Hey there  3600\n",
      "Hey there  3700\n",
      "Hey there  3800\n",
      "Hey there  3900\n",
      "Hey there  4000\n",
      "training set evaluated 4000\n",
      "Hey there  4100\n",
      "Hey there  4200\n",
      "Hey there  4300\n",
      "Hey there  4400\n",
      "Hey there  4500\n",
      "Hey there  4600\n",
      "Hey there  4700\n",
      "Hey there  4800\n",
      "Hey there  4900\n",
      "Hey there  5000\n",
      "training set evaluated 5000\n",
      "Hey there  5100\n",
      "Hey there  5200\n",
      "Hey there  5300\n",
      "Hey there  5400\n",
      "Hey there  5500\n",
      "Hey there  5600\n",
      "Hey there  5700\n",
      "Hey there  5800\n",
      "Hey there  5900\n",
      "Hey there  6000\n",
      "training set evaluated 6000\n",
      "Hey there  6100\n",
      "Hey there  6200\n",
      "Hey there  6300\n",
      "Hey there  6400\n",
      "Hey there  6500\n",
      "Hey there  6600\n",
      "Hey there  6700\n",
      "Hey there  6800\n",
      "Hey there  6900\n",
      "Hey there  7000\n",
      "training set evaluated 7000\n",
      "Hey there  7100\n",
      "Hey there  7200\n",
      "Hey there  7300\n",
      "Hey there  7400\n",
      "Hey there  7500\n",
      "Hey there  7600\n",
      "Hey there  7700\n",
      "Hey there  7800\n",
      "Hey there  7900\n",
      "Hey there  8000\n",
      "training set evaluated 8000\n",
      "Hey there  8100\n",
      "Hey there  8200\n",
      "Hey there  8300\n",
      "Hey there  8400\n",
      "Hey there  8500\n",
      "Hey there  8600\n",
      "Hey there  8700\n",
      "Hey there  8800\n",
      "Hey there  8900\n",
      "Hey there  9000\n",
      "training set evaluated 9000\n",
      "Hey there  9100\n",
      "Hey there  9200\n",
      "Hey there  9300\n",
      "Hey there  9400\n",
      "Hey there  9500\n",
      "Hey there  9600\n",
      "Hey there  9700\n",
      "Hey there  9800\n",
      "Hey there  9900\n",
      "Hey there  10000\n",
      "training set evaluated 10000\n",
      "Hey there  10100\n",
      "Hey there  10200\n",
      "Hey there  10300\n",
      "Hey there  10400\n",
      "Hey there  10500\n",
      "Hey there  10600\n",
      "Hey there  10700\n",
      "Hey there  10800\n",
      "Hey there  10900\n",
      "Hey there  11000\n",
      "training set evaluated 11000\n",
      "Hey there  11100\n",
      "Hey there  11200\n",
      "Hey there  11300\n",
      "Hey there  11400\n",
      "Hey there  11500\n",
      "Hey there  11600\n",
      "Hey there  11700\n",
      "Hey there  11800\n",
      "Hey there  11900\n",
      "Hey there  12000\n",
      "training set evaluated 12000\n",
      "Hey there  12100\n",
      "Hey there  12200\n",
      "Hey there  12300\n",
      "Hey there  12400\n",
      "Hey there  12500\n",
      "Hey there  12600\n",
      "Hey there  12700\n",
      "Hey there  12800\n",
      "Hey there  12900\n",
      "Hey there  13000\n",
      "training set evaluated 13000\n",
      "Hey there  13100\n",
      "Hey there  13200\n",
      "Hey there  13300\n",
      "Hey there  13400\n",
      "Hey there  13500\n",
      "Hey there  13600\n",
      "Hey there  13700\n",
      "Hey there  13800\n",
      "Hey there  13900\n",
      "Hey there  14000\n",
      "training set evaluated 14000\n",
      "Hey there  14100\n",
      "Hey there  14200\n",
      "Hey there  14300\n",
      "Hey there  14400\n",
      "Hey there  14500\n",
      "Hey there  14600\n",
      "Hey there  14700\n",
      "Hey there  14800\n",
      "Hey there  14900\n",
      "Hey there  15000\n",
      "training set evaluated 15000\n",
      "Hey there  15100\n",
      "Hey there  15200\n",
      "Hey there  15300\n",
      "Hey there  15400\n",
      "Hey there  15500\n",
      "Hey there  15600\n",
      "Hey there  15700\n",
      "Hey there  15800\n",
      "Hey there  15900\n",
      "Hey there  16000\n",
      "training set evaluated 16000\n",
      "Hey there  16100\n",
      "Hey there  16200\n",
      "Hey there  16300\n",
      "Hey there  16400\n",
      "Hey there  16500\n",
      "Hey there  16600\n",
      "Hey there  16700\n",
      "Hey there  16800\n",
      "Hey there  16900\n",
      "Hey there  17000\n",
      "training set evaluated 17000\n",
      "Hey there  17100\n",
      "Hey there  17200\n",
      "Hey there  17300\n",
      "Hey there  17400\n",
      "Hey there  17500\n",
      "Hey there  17600\n",
      "Hey there  17700\n",
      "Hey there  17800\n",
      "Hey there  17900\n",
      "Hey there  18000\n",
      "training set evaluated 18000\n",
      "Hey there  18100\n",
      "Hey there  18200\n",
      "Hey there  18300\n",
      "Hey there  18400\n",
      "Hey there  18500\n",
      "Hey there  18600\n",
      "Hey there  18700\n",
      "Hey there  18800\n",
      "Hey there  18900\n",
      "Hey there  19000\n",
      "training set evaluated 19000\n",
      "Hey there  19100\n",
      "Hey there  19200\n",
      "Hey there  19300\n",
      "Hey there  19400\n",
      "Hey there  19500\n",
      "Hey there  19600\n",
      "Hey there  19700\n",
      "Hey there  19800\n",
      "Hey there  19900\n",
      "Hey there  20000\n",
      "training set evaluated 20000\n",
      "Hey there  20100\n",
      "Hey there  20200\n",
      "Hey there  20300\n",
      "Hey there  20400\n",
      "Hey there  20500\n",
      "Hey there  20600\n",
      "Hey there  20700\n",
      "Hey there  20800\n",
      "Hey there  20900\n",
      "Hey there  21000\n",
      "training set evaluated 21000\n",
      "Hey there  21100\n",
      "Hey there  21200\n",
      "Hey there  21300\n",
      "Hey there  21400\n",
      "Hey there  21500\n",
      "Hey there  21600\n",
      "Hey there  21700\n",
      "Hey there  21800\n",
      "Hey there  21900\n",
      "Hey there  22000\n",
      "training set evaluated 22000\n",
      "Hey there  22100\n",
      "Hey there  22200\n",
      "Hey there  22300\n",
      "Hey there  22400\n",
      "Hey there  22500\n",
      "Hey there  22600\n",
      "Hey there  22700\n",
      "Hey there  22800\n",
      "Hey there  22900\n",
      "Hey there  23000\n",
      "training set evaluated 23000\n",
      "Hey there  23100\n",
      "Hey there  23200\n",
      "Hey there  23300\n",
      "Hey there  23400\n",
      "Hey there  23500\n",
      "Hey there  23600\n",
      "Hey there  23700\n",
      "Hey there  23800\n",
      "Hey there  23900\n",
      "Hey there  24000\n",
      "training set evaluated 24000\n",
      "Hey there  24100\n",
      "Hey there  24200\n",
      "Hey there  24300\n",
      "Hey there  24400\n",
      "Hey there  24500\n",
      "Hey there  24600\n",
      "Hey there  24700\n",
      "Hey there  24800\n",
      "Hey there  24900\n",
      "Hey there  25000\n",
      "training set evaluated 25000\n",
      "Hey there  25100\n",
      "Hey there  25200\n",
      "Hey there  25300\n",
      "Hey there  25400\n",
      "Hey there  25500\n",
      "Hey there  25600\n",
      "Hey there  25700\n",
      "Hey there  25800\n",
      "Hey there  25900\n",
      "Hey there  26000\n",
      "training set evaluated 26000\n",
      "Hey there  26100\n",
      "Hey there  26200\n",
      "Hey there  26300\n",
      "Hey there  26400\n",
      "Hey there  26500\n",
      "Hey there  26600\n",
      "Hey there  26700\n",
      "Hey there  26800\n",
      "Hey there  26900\n",
      "Hey there  27000\n",
      "training set evaluated 27000\n",
      "Hey there  27100\n",
      "Hey there  27200\n",
      "Hey there  27300\n",
      "Hey there  27400\n",
      "Hey there  27500\n",
      "Hey there  27600\n",
      "Hey there  27700\n",
      "Hey there  27800\n",
      "Hey there  27900\n",
      "Hey there  28000\n",
      "training set evaluated 28000\n",
      "Hey there  28100\n",
      "Hey there  28200\n",
      "Hey there  28300\n",
      "Hey there  28400\n",
      "Hey there  28500\n",
      "Hey there  28600\n",
      "Hey there  28700\n",
      "Hey there  28800\n",
      "Hey there  28900\n",
      "Hey there  29000\n",
      "training set evaluated 29000\n",
      "Hey there  29100\n",
      "Hey there  29200\n",
      "Hey there  29300\n",
      "Hey there  29400\n",
      "Hey there  29500\n",
      "Hey there  29600\n",
      "Hey there  29700\n",
      "Hey there  29800\n",
      "Hey there  29900\n",
      "Hey there  30000\n",
      "training set evaluated 30000\n",
      "Hey there  30100\n",
      "Hey there  30200\n",
      "Hey there  30300\n",
      "Hey there  30400\n",
      "Hey there  30500\n",
      "Hey there  30600\n",
      "Hey there  30700\n",
      "Hey there  30800\n",
      "Hey there  30900\n",
      "Hey there  31000\n",
      "training set evaluated 31000\n",
      "Hey there  31100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there  31200\n",
      "Hey there  31300\n",
      "Hey there  31400\n",
      "Hey there  31500\n",
      "Hey there  31600\n",
      "Hey there  31700\n",
      "Hey there  31800\n",
      "Hey there  31900\n",
      "Hey there  32000\n",
      "training set evaluated 32000\n",
      "Hey there  32100\n",
      "Hey there  32200\n",
      "Hey there  32300\n",
      "Hey there  32400\n",
      "Hey there  32500\n",
      "Hey there  32600\n",
      "Hey there  32700\n",
      "Hey there  32800\n",
      "Hey there  32900\n",
      "Hey there  33000\n",
      "training set evaluated 33000\n",
      "Hey there  33100\n",
      "Hey there  33200\n",
      "Hey there  33300\n",
      "Hey there  33400\n",
      "Hey there  33500\n",
      "Hey there  33600\n",
      "Hey there  33700\n",
      "Hey there  33800\n",
      "Hey there  33900\n",
      "Hey there  34000\n",
      "training set evaluated 34000\n",
      "Hey there  34100\n",
      "Hey there  34200\n",
      "Hey there  34300\n",
      "Hey there  34400\n",
      "Hey there  34500\n",
      "Hey there  34600\n",
      "Hey there  34700\n",
      "Hey there  34800\n",
      "Hey there  34900\n",
      "Hey there  35000\n",
      "training set evaluated 35000\n",
      "Hey there  35100\n",
      "Hey there  35200\n",
      "Hey there  35300\n",
      "Hey there  35400\n",
      "model trained\n",
      "validation set evaluated 1000\n",
      "validation set evaluated 2000\n",
      "validation set evaluated 3000\n",
      "validation set evaluated 4000\n",
      "validation set evaluated 5000\n",
      "validation set evaluated 6000\n",
      "validation set evaluated 7000\n",
      "validation set evaluated 8000\n",
      "validation set evaluated 9000\n",
      "validation set evaluated 10000\n",
      "validation set evaluated 11000\n",
      "validation set evaluated 12000\n",
      "validation set evaluated 13000\n",
      "validation set evaluated 14000\n",
      "validation set evaluated 15000\n",
      "testing set evaluated 1000\n",
      "testing set evaluated 2000\n",
      "testing set evaluated 3000\n",
      "testing set evaluated 4000\n",
      "testing set evaluated 5000\n",
      "testing set evaluated 6000\n",
      "testing set evaluated 7000\n",
      "testing set evaluated 8000\n",
      "testing set evaluated 9000\n",
      "testing set evaluated 10000\n",
      "testing set evaluated 11000\n",
      "testing set evaluated 12000\n",
      "testing set evaluated 13000\n",
      "testing set evaluated 14000\n",
      "testing set evaluated 15000\n",
      "testing set evaluated 16000\n",
      "testing set evaluated 17000\n",
      "testing set evaluated 18000\n",
      "testing set evaluated 19000\n",
      "testing set evaluated 20000\n",
      "testing set evaluated 21000\n",
      "accuracy 0.028266\n",
      "ctr 0.019822644022200752\n",
      "precision\n",
      "0.028266273823773135\n",
      "NDGC\n",
      "0.04077961306996635\n"
     ]
    }
   ],
   "source": [
    "target_evaluator.evaluate(smp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-d2b7a3f08c64>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-d2b7a3f08c64>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    batch size 1000\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "batch size 1000\n",
    "iterations :10 \n",
    "    _____________\n",
    "accuracy 0.250963\n",
    "ctr 0.04623804173875244\n",
    "precision\n",
    "0.25096348791758755\n",
    "NDGC\n",
    "0.362063779462863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch size : 500\n",
    "iterations : 50\n",
    "    \n",
    "    ____________________\n",
    "    \n",
    "accuracy 0.410747\n",
    "ctr 0.14885359288583871\n",
    "precision\n",
    "0.4107472282598769\n",
    "NDGC\n",
    "0.5925829892693465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy 0.583919\n",
    "ctr 0.20346427770087075\n",
    "precision\n",
    "0.5839193409925295\n",
    "NDGC\n",
    "0.8424175375289795\n",
    "\n",
    "3 layer\n",
    "batch size=100\n",
    "iterations :50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "3 layer 120 -80-40\n",
    "batch size : 100\n",
    "iter = 50\n",
    "\n",
    "\n",
    "accuracy 0.582385\n",
    "ctr 0.22105107629248555\n",
    "precision\n",
    "0.5823850663939067\n",
    "NDGC\n",
    "0.8402040471741848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy 0.589709\n",
    "ctr 0.20182517504152558\n",
    "precision\n",
    "0.5897094010849513\n",
    "NDGC\n",
    "0.8507708285107646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM\n",
    "batch size 80\n",
    "encoding to 32\n",
    "\n",
    "accuracy 0.505872\n",
    "ctr 0.15674250904718953\n",
    "precision\n",
    "0.5058722533744908\n",
    "NDGC\n",
    "0.729819391266622"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM\n",
    "Batch size 120\n",
    "encoding 16\n",
    "\n",
    "accuracy 0.541014\n",
    "ctr 0.18200622240655773\n",
    "precision\n",
    "0.5410144477524703\n",
    "NDGC\n",
    "0.7805188608216831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM\n",
    "Batch size 120\n",
    "encoding 16\n",
    "simultaneous updates\n",
    "\n",
    "accuracy 0.570622\n",
    "ctr 0.18662399152101622\n",
    "precision\n",
    "0.5706222944711319\n",
    "NDGC\n",
    "0.8232339544540916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM\n",
    "Batch size 240\n",
    "encoding 16\n",
    "simultaneous updates\n",
    "\n",
    "accuracy 0.648834\n",
    "ctr 0.1964724422136122\n",
    "precision\n",
    "0.6488337686533088\n",
    "NDGC\n",
    "0.93606926039732"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

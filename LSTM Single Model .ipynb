{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# from eval import liHongEvaluator\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# from neural_model import TFNeuralNetworkRegressor\n",
    "# from get_onehotencoded_structure import get_structure1,dict_arm_to_structure1\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from math import log\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import json\n",
    "from keras.layers import LSTM\n",
    "# from conversions import get_structure1,dict_arm_to_structure1\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "# from getData import campaign_def\n",
    "import keras\n",
    "#from getData import campaign_def\n",
    "from datetime import datetime\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import RepeatVector\n",
    "from tflearn import DNN\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected \n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class liHongEvaluator():\n",
    "\n",
    "    def __init__(self, main_folder, output_column_name, print_params = {}, use_non_stationarity = False, sampling_fraction=0.1, stochastic_param = [1,0]):\n",
    "        self.main_folder = main_folder\n",
    "        self.df_original = pd.read_csv(main_folder,engine='python')\n",
    "#         self.df_original=self.df_original.drop(columns=['user_id','article_id'])\n",
    "        print(self.df_original.head())\n",
    "\n",
    "        all_columns = list(self.df_original.columns)\n",
    "        self.output_column_names = ['words_count',\n",
    "       'category_26', 'category_66', 'category_136', 'category_186',\n",
    "       'category_209', 'category_247', 'category_250', 'category_265',\n",
    "       'category_281', 'category_301', 'category_327', 'category_331',\n",
    "       'category_348', 'category_351', 'category_354', 'category_375',\n",
    "       'category_409', 'category_412', 'category_418', 'category_431',\n",
    "       'category_437','29953','39894','70591','87224','95972','96663','119592',\n",
    "       '123290','123818','123826','124176','140528','156625','158082','160474',\n",
    "       '160940','161425','182394','202557','206934','214800','215861','225019','225055',\n",
    "                                    '235132','236207','236338','236444','236951','276946',\n",
    "                                    '283392','283505','284096','284583','285343','285377',\n",
    "                                    '288435','313920','337143','338339','created_at_ts']\n",
    "\n",
    "         \n",
    "\n",
    "        self.input_columns = list(set(all_columns) - set(self.output_column_names))\n",
    "        \n",
    "#         print(self.input_columns)\n",
    "        self.arm_column_names = self.output_column_names\n",
    "        self.logging_frequency = 1000\n",
    "        self.number_features_in_context = len(self.input_columns)\n",
    "        self.no_oflist_arms = len(self.output_column_names)\n",
    "        total_arms =40\n",
    "\n",
    "        #For testing, Comment out\n",
    "        self.count_not_found = 0\n",
    "        self.found_in_exclusion = 0\n",
    "        self.found_in_set = 0\n",
    "        self.flag = True\n",
    "        x = self.df_original[self.input_columns].values\n",
    "        x_scaled=pd.DataFrame(x,columns = self.input_columns)\n",
    "        self.unique = self.df_original[self.output_column_names].drop_duplicates()\n",
    "        self.unique = self.unique.reset_index(drop=True)\n",
    "\n",
    "        self.y=self.df_original[self.output_column_names].values\n",
    "        #To simulate stochasticity set this paramater\n",
    "        self.stochastic_param = stochastic_param\n",
    "        x_tr, x_test , y_tr, y_test = train_test_split(x_scaled,self.y,test_size=0.3,shuffle=False)\n",
    "        x_train ,x_valid,y_train, y_valid = train_test_split(x_tr,y_tr,test_size=0.3,shuffle=False)\n",
    " \n",
    "        self.x_training = x_train.values.tolist() #list(map(float,x_train.values))#pd.DataFrame(x_tr, columns = self.input_columns)\n",
    "        self.y_training = y_train #pd.DataFrame(y_tr, columns = [self.output_column_names])\n",
    "        self.x_valid=x_valid.values.tolist()\n",
    "        self.y_valid=y_valid\n",
    "        self.x_testing =x_test.values.tolist()#pd.DataFrame(x_test, columns = self.input_columns)\n",
    "        self.y_testing = y_test# pd.DataFrame(y_test, columns = [self.output_column_names])\n",
    "        x_train_df = pd.DataFrame(x_tr, columns = self.input_columns)\n",
    "        x_test_df = pd.DataFrame(x_test, columns = self.input_columns)\n",
    "        \n",
    "        self.e=0.2\n",
    "        self.decay=0.998\n",
    "        \n",
    "        print ('x_traing',self.x_training[0])\n",
    "        print ('x_valid',self.x_valid[0])\n",
    "        print ('x_test' ,self.x_testing[0])\n",
    "\n",
    "\n",
    "        random.seed(9001)\n",
    "\n",
    "        self.non_stationarity_steps = 25000\n",
    "        \n",
    "    def get_size_input(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def get_no_oflist_arms(self):\n",
    "        return self.no_oflist_arms\n",
    "\n",
    "    def get_dataset_shape(self):\n",
    "        return self.df.shape\n",
    "\n",
    "    def get_number_features_in_context(self):\n",
    "        return self.number_features_in_context\n",
    "\n",
    "    #rename to get_one_hot_structure_array\n",
    "    def get_structure(self):\n",
    "#         print(np.unique(self.y))\n",
    "        structure=self.unique\n",
    "        #print 'structure', structure\n",
    "        return structure\n",
    "    #rename to get_one_hot_structure\n",
    "    def dict_arm_to_structure(self):\n",
    "        dict_str ={}\n",
    "        y_unique = self.get_structure()\n",
    "        print (y_unique.head())\n",
    "        for i ,r in y_unique.iterrows():\n",
    "            dict_str[i] = r.values.tolist()\n",
    "        return dict_str\n",
    "\n",
    "    def get_exclusions(self):\n",
    "        arms_set_dict ,self.exclusion_dict = campaign_def(self.main_folder)\n",
    "        return self.exclusion_dict\n",
    "\n",
    "\n",
    "    def evaluate(self,policy):\n",
    "        \n",
    "        dict_arm_structure = self.dict_arm_to_structure()\n",
    "        score=0\n",
    "        total_arms=40\n",
    "        start_time = datetime.now()\n",
    "        q=0\n",
    "        res_rew=[]\n",
    "        rew_est=0\n",
    "        \n",
    "\n",
    "        \n",
    "        print(\"now training model\")\n",
    "        for state, right_arm in zip(self.x_training,self.y_training):\n",
    "            \n",
    "            q+=1\n",
    "            if(q%100 == 0):\n",
    "                res_rew.append(rew_est)\n",
    "                rew_est=0\n",
    "            \n",
    "            \n",
    "            import random\n",
    "            rand = random.uniform(0, 1)\n",
    "\n",
    "            if(rand < self.e):\n",
    "                self.e=self.e*self.decay\n",
    "                selected_action =[]\n",
    "                \n",
    "                while len(selected_action) <1:\n",
    "                    temp = self.unique.iloc[np.random.randint(0,total_arms)].values.tolist()\n",
    "                    if(temp not in selected_action):\n",
    "                        selected_action.append(temp)\n",
    "            else:        \n",
    "            \n",
    "                selected_action = policy.get_action(state)\n",
    "                \n",
    "            reward=0\n",
    "\n",
    "            for arm in selected_action :\n",
    "                reward=0\n",
    "                if arm == right_arm.tolist():\n",
    "\n",
    "                    reward = 1\n",
    "                    rew_est+=1\n",
    "\n",
    "                state2 =list(state) + (arm)\n",
    "                policy.update_model(reward,state2,arm,True)\n",
    "\n",
    "            \n",
    "            #print(\"!!!\", right_arm)\n",
    "#             state1 = list(state) + list(right_arm)\n",
    "#             policy.update_model(1,state1,right_arm,True)\n",
    "            \n",
    "#             import random\n",
    "#             random_sample = list(self.unique.iloc[random.randint(0,39)])\n",
    "#             if random_sample!=list(right_arm):\n",
    "#                 policy.update_model(0,state + random_sample,random_sample, True)\n",
    "\n",
    "#             pairs = list(zip(self.x_training,self.y_training))  # make pairs out of the two lists\n",
    "#             pairs = random.sample(pairs, 1)  # pick 3 random pairs\n",
    "#             A1, B1 = zip(*pairs)  # separate the pairs\n",
    "#             for k in range(1):\n",
    "#                 if(list(B1[k])!=list(right_arm)):\n",
    "#                     state1 = list(A1[k]) + list(B1[k])\n",
    "#                     policy.update_model(0,state1,B1[k],True)\n",
    "                    \n",
    "            \n",
    "#             for index, row in self.unique.iterrows():\n",
    "#                 row = row.to_numpy()\n",
    "#                 state2 =list( state) + list(row)\n",
    "#                 #print(state)\n",
    "#                 if(list(row)==list(right_arm)):\n",
    "#                     policy.update_model(1,state2,right_arm,True)\n",
    "#                 else:\n",
    "#                     policy.update_model(0,state2,row,True)\n",
    "            \n",
    "# # # #             print('state',state)\n",
    "            \n",
    "# # #         policy.train_model(self.x_training, self.y_training)\n",
    "        print(\"model trained\")\n",
    "        \n",
    "        for state, right_arm in zip(self.x_valid, self.y_valid):\n",
    "            \n",
    "            q+=1\n",
    "            if(q%100 == 0):\n",
    "                res_rew.append(rew_est)\n",
    "                rew_est=0\n",
    "\n",
    "            import random\n",
    "            rand = random.uniform(0, 1)\n",
    "\n",
    "            if(rand < self.e):\n",
    "                self.e=self.e*self.decay\n",
    "                selected_action =[]\n",
    "\n",
    "                while len(selected_action) <1:\n",
    "                    temp = self.unique.iloc[np.random.randint(0,total_arms)].values.tolist()\n",
    "                    if(temp not in selected_action):\n",
    "                        selected_action.append(temp)\n",
    "            else:\n",
    "                selected_action = policy.get_action(state)\n",
    "                \n",
    "            reward=0\n",
    "\n",
    "\n",
    "            for arm in selected_action :\n",
    "                reward=0\n",
    "                if arm == right_arm.tolist():\n",
    "\n",
    "                    reward = 1\n",
    "                    rew_est+=1\n",
    "\n",
    "\n",
    "                state2 =list(state) + (arm)\n",
    "                policy.update_model(reward,state2,arm,True)\n",
    "\n",
    "        \n",
    "        \n",
    "##############################################TESTING##############################################\n",
    "        \n",
    "        \n",
    "        count =0\n",
    "        \n",
    "        precision=0\n",
    "        dict_recall= {}\n",
    "        dict_impressions = {}\n",
    "        dict_clicks = {}\n",
    "        \n",
    "        for k in self.unique.values:\n",
    "            dict_impressions[tuple(k)] =0\n",
    "            dict_clicks[tuple(k)]=0\n",
    "        q=0\n",
    "        ndgc=0\n",
    "        rew_est =0\n",
    "        res_rew=[]\n",
    "        for state, right_arm in zip(self.x_testing, self.y_testing):\n",
    "                                    \n",
    "            import random\n",
    "            rand = random.uniform(0, 1)\n",
    "\n",
    "            if(rand < self.e):\n",
    "                self.e=self.e*self.decay\n",
    "                selected_action =[]\n",
    "\n",
    "                while len(selected_action) <1:\n",
    "                    temp = self.unique.iloc[np.random.randint(0,total_arms)].values.tolist()\n",
    "                    if(temp not in selected_action):\n",
    "                        selected_action.append(temp)\n",
    "            else:\n",
    "                selected_action = policy.get_action(state)\n",
    "                \n",
    "                \n",
    "            q+=1\n",
    "            if(q%100 == 0):\n",
    "                res_rew.append(rew_est)\n",
    "                rew_est=0\n",
    "            \n",
    "\n",
    "\n",
    "            y_value = right_arm\n",
    "    \n",
    "            \n",
    "\n",
    "            count=count+1\n",
    "            self.stime = datetime.now()\n",
    "            if count % self.logging_frequency == 0 :\n",
    "                print('evaluated %d' % count)\n",
    "# #             print('initial', len(state))\n",
    "            selected_action = policy.get_action(state)\n",
    "#             print(\"\")\n",
    "            match_found = False\n",
    "            ps =0\n",
    "            ps_sum=0\n",
    "            i=0\n",
    "            num_ndgc=0\n",
    "            d_ndgc=0\n",
    "            k_ndgc=0\n",
    "            for arm in selected_action : \n",
    "                    reward=0\n",
    "                    dict_impressions[tuple(arm)]+=1\n",
    "\n",
    "                    if arm == right_arm.tolist():\n",
    "                        rew_est +=1\n",
    "\n",
    "                        match_found = True\n",
    "                        reward = 1\n",
    "                        score = score+1\n",
    "\n",
    "                        ps+=1\n",
    "                        ps_sum += ps/(i+1)\n",
    "                        i=i+1\n",
    "                        \n",
    "                        if(i+1==1):\n",
    "                            d_ndgc+=1\n",
    "                        else:\n",
    "                            d_ndgc+=1/(log(i+1))\n",
    "                        num_ndgc = num_ndgc+1\n",
    "                        \n",
    "                        dict_clicks[tuple(y_value)] +=1\n",
    "\n",
    "\n",
    "                    state2 =list(state) + (arm)\n",
    "                    policy.update_model(reward,state2,arm,True)\n",
    "                    \n",
    "            if(match_found):\n",
    "                precision =precision + ps_sum\n",
    "                \n",
    "            for i in range(num_ndgc):\n",
    "                if(i+1 == 1):\n",
    "                    k_ndgc+=1\n",
    "                else:\n",
    "                    k_ndgc+= 1/(log(i+1))\n",
    "            \n",
    "            if(k_ndgc!=0):\n",
    "                ndgc = ndgc + (d_ndgc/k_ndgc)\n",
    "            \n",
    "\n",
    "\n",
    "        print('accuracy %f' % ( float(score) / count ))\n",
    "        \n",
    "        ctr = 0 \n",
    "        for key, value in dict_impressions.items():\n",
    "            if(value!=0):\n",
    "                ctr += dict_clicks[key]/value\n",
    "        print('ctr',ctr/len(dict_impressions.keys()))\n",
    "        \n",
    "        with open('result_reward_single_news_ts1.pickle', 'wb') as handle:\n",
    "            pickle.dump(res_rew, handle)\n",
    "        \n",
    "        print (\"precision\")\n",
    "        print (precision/len(self.x_testing))\n",
    "        \n",
    "                \n",
    "        print (\"NDGC\")\n",
    "        print (ndgc/len(self.x_testing))\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/anubha/Seagate Expansion Drive/china'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model\n",
    "data_path = '/media/anubha/Seagate Expansion Drive/china'\n",
    "class TFNeuralNetworkRegressor():\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def __init__(self, number_features_in_context, number_features_in_structure):\n",
    "        \n",
    "        \n",
    "        self.pre_model = Sequential()\n",
    "        self.pre_model.add(LSTM(32, batch_input_shape=(1,1,number_features_in_context+number_features_in_structure),stateful=True,name='layer1'))\n",
    "        self.pre_model.add(RepeatVector(1))\n",
    "        self.pre_model.add(LSTM((number_features_in_context+number_features_in_structure), return_sequences=True)) \n",
    "        self.pre_model.add(Dense((number_features_in_context+number_features_in_structure),activation ='softmax'))\n",
    "        self.pre_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "#         checkpointer = ModelCheckpoint(filepath=data_path + '/model-{epoch:02d}.hdf5', verbose=0)\n",
    "        print(self.pre_model.summary())\n",
    "    \n",
    "        self.model = Sequential()\n",
    "        #First Hidden Layer\n",
    "#         self.model.add(LSTM(120, batch_input_shape=(100,number_features_in_context+number_features_in_structure,1), stateful=True))\n",
    "\n",
    "        self.model.add(Dense(120, activation='tanh', kernel_initializer='random_normal', input_shape=(32,)))\n",
    "        \n",
    "        \n",
    "        self.model.add(Dense(80, activation='tanh', kernel_initializer='random_normal'))\n",
    "        \n",
    "        self.model.add(Dense(40, activation='tanh', kernel_initializer='random_normal'))\n",
    "        \n",
    "\n",
    "        self.model.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
    "        # For a binary classification problem\n",
    "        \n",
    "        self.model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    \n",
    "        layer_name = 'layer1'\n",
    "        self.intermediate_layer_model = Model(inputs=self.pre_model.input,outputs=self.pre_model.get_layer(layer_name).output)\n",
    "    \n",
    "       \n",
    "    def fit(self, X, Y):\n",
    "#         print(X.shape)\n",
    "        X = X.reshape(X.shape[0],1,X.shape[1])\n",
    "#         intermediate_output = self.intermediate_layer_model.predict(data)\n",
    "        self.pre_model.fit(X,X,batch_size=1,verbose=0)\n",
    "        intermediate_output = self.intermediate_layer_model.predict(X,batch_size=1)\n",
    "        \n",
    "#         intermediate_from_pre_model = self.pre_model.get_layer('layer1').output\n",
    "        \n",
    "#         print(intermediate_from_pre_model)\n",
    "#         inp_s = len(intermediate_from_pre_model[0])\n",
    "#         inter  = intermediate_from_pre_model.tolist().reshape(X.shape[0], X.shape[1])\n",
    "        self.model.fit(intermediate_output, Y, epochs=5, verbose=0)\n",
    "        \n",
    "        \n",
    "   \n",
    "    def partial_fit(self, X,Y):\n",
    "        \n",
    "        \n",
    "        self.model.fit(X, Y, epochs=1, verbose=0)\n",
    "   \n",
    "    def predict(self, X):\n",
    "#         print('shape', X.shape)\n",
    "        X = X.reshape(X.shape[0], 1,X.shape[1])\n",
    "        intermediate_output = self.intermediate_layer_model.predict(X,batch_size=1)\n",
    "        \n",
    "#         intermediate_from_pre_model = self.pre_model.get_layer('lstm_22').output\n",
    "        \n",
    "        return self.model.predict(intermediate_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_arm_to_structure1():\n",
    "    with open('total_comb_structured_dict') as f:\n",
    "        data = json.load(f)\n",
    "        data1={}\n",
    "        for i in data.keys():\n",
    "            j=str(i)\n",
    "            data1[j]=data[i]\n",
    "        return data1\n",
    "\n",
    "def get_structure1():\n",
    "\ttotal_comb_structured_dict=dict_arm_to_structure1()\n",
    "\ttotal_structure_array= np.asarray(list(total_comb_structured_dict.values()))\n",
    "\treturn total_structure_array\n",
    "\n",
    "def num_to_key():\n",
    "    with open('dict_structure_to_arm') as f:\n",
    "        data = json.load(f)\n",
    "        data1={}\n",
    "        for i in data.keys():\n",
    "\n",
    "            j=str(i)\n",
    "            data1[j]=data[i]\n",
    "        return data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#150 -50-0.7 [150 - 100-50]\n",
    "class single_model_policy():\n",
    "    #Taking batch sizes of 1000\n",
    "    def __init__(self, number_features_in_context, structure, epsilon, training_dropout, prediction_dropout,batch_size = 100):\n",
    "        self.last_action = None\n",
    "        self.last_reward = None\n",
    "        self.last_state = None\n",
    "        self.num_arms = structure.shape[0]\n",
    "        self.structure_features = structure.shape[1]\n",
    "        self.epsilon = epsilon\n",
    "        self.experience = {}\n",
    "        self.i=0\n",
    "        self.total_features = number_features_in_context + self.structure_features\n",
    "        self.train_batch_size=80\n",
    "        self.mini_batch=500\n",
    "        dict_str ={}\n",
    "        self.cnt=0\n",
    "      \n",
    "        for i ,r in structure.iterrows():\n",
    "            dict_str[i] = r.values.tolist()\n",
    "            \n",
    "        \n",
    "     \n",
    "    \n",
    "        self.structure = dict_str\n",
    "#         print (self.structure)\n",
    "#         self.batch_size = batch_size\n",
    "        self.last_s=[]\n",
    "        self.last_r=[]\n",
    "        #if self.batch_size:\n",
    "        self.iter = 0\n",
    "        self.dict_update_hist = {}\n",
    "#         print(self.structure)\n",
    "        # self.model=TFNeuralNetworkRegressor(self.total_features, training_dropout, prediction_dropout)\n",
    "        self.model=TFNeuralNetworkRegressor(number_features_in_context, self.structure_features)\n",
    "        \n",
    "\n",
    "\n",
    "    #Choose best action for given context\n",
    "    def get_action(self, context):\n",
    "        #action should be category ID\n",
    "        best_action = self.get_best_arm(context)\n",
    "        arms=[]\n",
    "        for i in best_action:\n",
    "            arms.append(self.structure[i])\n",
    "            \n",
    "        return arms\n",
    "\n",
    "    #Loop over the structures of every arm for a given context and return the best prediction\n",
    "    def get_best_arm(self,context):\n",
    "\n",
    "        best_score = 0\n",
    "        best_arm = None\n",
    "\n",
    "        inputs = []\n",
    "        arms = []\n",
    "        for arm,structure in self.structure.items():\n",
    "#             print((structure))\n",
    "#             print (list())\n",
    "            state = list(context) + list(structure)\n",
    "#             print(len(state))\n",
    "            inputs.append(state)\n",
    "            arms.append(arm)\n",
    "\n",
    "        scores = [ elt[0] for elt in list(self.model.predict(np.array(inputs))) ]\n",
    "#         print(scores)\n",
    "        n=0\n",
    "        select_arm=[]\n",
    "        s = sorted(range(len(scores)),reverse = True,  key=lambda k: scores[k])\n",
    "\n",
    "        return s[:1]\n",
    "\n",
    "    \n",
    "    def train_model(self, x_train, y_train):\n",
    "        self.model.fit(x_train,y_train)\n",
    "    #Update model\n",
    "\n",
    "    def update_model(self, reward,last_state=[] ,last_action=[],update_in_train=False):\n",
    "        #Immediate Updates\n",
    "        if(len(last_action) != 0):\n",
    "            self.last_action = last_action\n",
    "        if(len(last_state) !=0):\n",
    "            self.last_state = last_state\n",
    "        self.last_reward = reward\n",
    "        self.model.fit(np.array([self.last_state]), np.array([[self.last_reward]]))\n",
    "#         print (batch_size)\n",
    "        if update_in_train:\n",
    "#             if(len(self.last_s)!=80):\n",
    "            if(self.iter == 0 or self.iter%self.train_batch_size != 0):\n",
    "#                 if(reward==0):\n",
    "#                     self.cnt+=1\n",
    "\n",
    "#                 if(self.cnt<int(0.5*self.train_batch_size) or reward==1):\n",
    "\n",
    "#                     if(self.last_state not in self.last_s):\n",
    "                self.last_s.append(self.last_state)\n",
    "                self.last_r.append(self.last_reward)\n",
    "\n",
    "            else:\n",
    "\n",
    "                for iteration in range(1,50):\n",
    "                    if(iteration%500==0):\n",
    "                        print(\"Iteration : %d\" % iteration)\n",
    "                    num_to_sample = min(len(self.last_s), self.mini_batch)\n",
    "                    random_indices=random.sample(range(0,len(self.last_s)),num_to_sample)\n",
    "                    x_train = []\n",
    "                    y_train = []\n",
    "                    for index in random_indices:\n",
    "\n",
    "                        x_train.append(self.last_s[index])\n",
    "                        y_train.append([self.last_r[index]])\n",
    "\n",
    "                    self.model.fit(np.array(x_train),np.array(y_train))\n",
    "                self.last_s=[]\n",
    "                self.last_r=[]\n",
    "                self.cnt=0\n",
    "            self.iter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   session_start  session_size  click_timestamp  click_region  created_at_ts  \\\n",
      "0       0.000986      0.000000         0.000000      0.888889        0.00271   \n",
      "1       0.000714      0.008197         0.000010      0.444444        0.00000   \n",
      "2       0.000933      0.016393         0.000024      1.000000        0.00000   \n",
      "3       0.000862      0.000000         0.000028      0.296296        0.00271   \n",
      "4       0.000351      0.000000         0.000035      0.740741        0.00271   \n",
      "\n",
      "   words_count  category_26  category_66  category_136  category_186  ...  \\\n",
      "0     0.543590            0            0             0             0  ...   \n",
      "1     0.712821            0            0             0             0  ...   \n",
      "2     0.712821            0            0             0             0  ...   \n",
      "3     0.543590            0            0             0             0  ...   \n",
      "4     0.543590            0            0             0             0  ...   \n",
      "\n",
      "   283392  283505  284096  284583  285343  285377  288435  313920  337143  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   338339  \n",
      "0       0  \n",
      "1       0  \n",
      "2       0  \n",
      "3       0  \n",
      "4       0  \n",
      "\n",
      "[5 rows x 101 columns]\n",
      "x_traing [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009864352523436537, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8888888888888888, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n",
      "x_valid [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.02459016393442624, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3281532660323592, 0.3385745974521797, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8888888888888888, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n",
      "x_test [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.08196721311475409, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5133445711824152, 0.5290237105261895, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4444444444444444, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]\n",
      "Created Evaluator\n"
     ]
    }
   ],
   "source": [
    "training_dropout, prediction_dropout =  0.5, 0.5\n",
    "epsilon = 0.00001\n",
    "\n",
    "main_folder = 'news_context_&_structure_TS_sorted.csv'\n",
    "\n",
    "target_evaluator = liHongEvaluator(main_folder, ['article_id'] , {}, False, 0.1, [1,0])\n",
    "\n",
    "print(\"Created Evaluator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(main_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure= target_evaluator.get_structure()\n",
    "number_features_in_context = target_evaluator.get_number_features_in_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "layer1 (LSTM)                (1, 32)                   17152     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (1, 1, 32)                0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, 1, 101)               54136     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 1, 101)               10302     \n",
      "=================================================================\n",
      "Total params: 81,590\n",
      "Trainable params: 81,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 120)               3960      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 80)                9680      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 40)                3240      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 16,921\n",
      "Trainable params: 16,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "smp = single_model_policy(number_features_in_context, structure, epsilon, training_dropout, prediction_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   words_count  category_26  category_66  category_136  category_186  \\\n",
      "0     0.543590            0            0             0             0   \n",
      "1     0.712821            0            0             0             0   \n",
      "2     0.866667            0            0             0             0   \n",
      "3     0.282051            0            0             0             0   \n",
      "4     1.000000            0            0             0             0   \n",
      "\n",
      "   category_209  category_247  category_250  category_265  category_281  ...  \\\n",
      "0             1             0             0             0             0  ...   \n",
      "1             0             1             0             0             0  ...   \n",
      "2             0             0             0             0             1  ...   \n",
      "3             0             0             0             0             0  ...   \n",
      "4             0             0             0             0             0  ...   \n",
      "\n",
      "   283505  284096  284583  285343  285377  288435  313920  337143  338339  \\\n",
      "0       0       0       0       0       0       0       0       0       0   \n",
      "1       0       0       0       0       0       0       0       0       0   \n",
      "2       0       0       0       0       0       0       0       0       0   \n",
      "3       0       0       0       0       0       0       0       0       0   \n",
      "4       0       0       0       0       0       0       0       0       0   \n",
      "\n",
      "   created_at_ts  \n",
      "0       0.002710  \n",
      "1       0.000000  \n",
      "2       0.036494  \n",
      "3       0.070352  \n",
      "4       0.126331  \n",
      "\n",
      "[5 rows x 63 columns]\n",
      "now training model\n",
      "WARNING:tensorflow:From /home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/anubha/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d615c7101286>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-f07e7ef908e0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mselected_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-07f9e3f0eb22>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#action should be category ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_arm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0marms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbest_action\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-07f9e3f0eb22>\u001b[0m in \u001b[0;36mget_best_arm\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0marms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0melt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;31m#         print(scores)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7eea36d86160>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#         print('shape', X.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_layer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m#         intermediate_from_pre_model = self.pre_model.get_layer('lstm_22').output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "target_evaluator.evaluate(smp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch size 1000\n",
    "iterations :10 \n",
    "    _____________\n",
    "accuracy 0.250963\n",
    "ctr 0.04623804173875244\n",
    "precision\n",
    "0.25096348791758755\n",
    "NDGC\n",
    "0.362063779462863"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch size : 500\n",
    "iterations : 50\n",
    "    \n",
    "    ____________________\n",
    "    \n",
    "accuracy 0.410747\n",
    "ctr 0.14885359288583871\n",
    "precision\n",
    "0.4107472282598769\n",
    "NDGC\n",
    "0.5925829892693465"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy 0.583919\n",
    "ctr 0.20346427770087075\n",
    "precision\n",
    "0.5839193409925295\n",
    "NDGC\n",
    "0.8424175375289795\n",
    "\n",
    "3 layer\n",
    "batch size=100\n",
    "iterations :50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "3 layer 120 -80-40\n",
    "batch size : 100\n",
    "iter = 50\n",
    "\n",
    "\n",
    "accuracy 0.582385\n",
    "ctr 0.22105107629248555\n",
    "precision\n",
    "0.5823850663939067\n",
    "NDGC\n",
    "0.8402040471741848"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy 0.589709\n",
    "ctr 0.20182517504152558\n",
    "precision\n",
    "0.5897094010849513\n",
    "NDGC\n",
    "0.8507708285107646"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
